{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViUniT Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /export/einstein-vision-hs/visual_unit_testing/\n",
    "import os\n",
    "os.environ['PYTHONPATH'] = '/nlpgpu/data/artemisp/visual_unit_testing:$CONDA_PREFIX/'\n",
    "os.environ['HF_HOME'] = '/nlpgpu/data/artemisp/.cache/huggingface'\n",
    "os.environ['TORCH_HOME'] = '/nlpgpu/data/artemisp/visual_unit_testing/.cache/'\n",
    "os.environ['HF_ACCESS_TOKEN'] = '<HF_TOKEN>'\n",
    "os.environ['HF_TOKEN'] = '<HF_TOKEN>'\n",
    "os.environ['CUDA_HOME'] = os.environ['CONDA_PREFIX']\n",
    "os.environ['CONFIG_NAMES'] = 'demo_config'\n",
    "os.environ[\"GQA_IMAGE_PATH\"] = \"/nlp/data/vision_datasets/GQA\"\n",
    "os.environ[\"WINOGROUND_IMAGE_PATH\"] = \"/nlp/data/vision_datasets/winoground/data/images\"\n",
    "os.environ[\"COCO_VAL2017\"] = \"/nlp/data/vision_datasets/winoground/data/images\"\n",
    "plot_path = '/nlpgpu/data/artemisp/visual_unit_testing/plots/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Imports and ImagePatch API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "import omegaconf\n",
    "from unit_test_generation.processing import extract_unit_tests, get_unit_test_prompt, get_grounded_diffusion_prompt\n",
    "from unit_test_generation.unit_test_sampling import TextSampler\n",
    "from utils import (load_config,\n",
    "                    get_base_prompt,\n",
    "                    get_visual_program_prompt,\n",
    "                    extract_python_code,\n",
    "                    get_visual_program_correction_prompt,\n",
    "                    SynonymChecker,\n",
    "                    set_seed,\n",
    "                    get_fixed_code,\n",
    "                    SYNTAX_ERRORS,\n",
    "                    initialize_image_generator\n",
    "                    )\n",
    "from viper_configs import viper_config\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "import copy\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "base_config = '/export/einstein-vision-hs/visual_unit_testing/viunit_configs/base.yaml'\n",
    "this_config = omegaconf.OmegaConf.load(base_config)\n",
    "\n",
    "\n",
    "llm_model_name = viper_config.llm.model_id\n",
    "codex_model_name = viper_config.codex.codellama_model_name\n",
    "# unit test generation\n",
    "unit_test_system_prompt = open(\n",
    "    this_config['unit_test_generation']['generation']['prompt_file']).read()\n",
    "unit_test_in_context_examples = open(\n",
    "    this_config['unit_test_generation']['generation']['in_context_examples_file']).read()\n",
    "    \n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    llm_model_name, trust_remote_code=True, token=os.getenv('HF_ACCESS_TOKEN'),)\n",
    "\n",
    "correction_prompt = open(\n",
    "this_config['visual_program_generator']['generation']['correction_prompt_file']).read()\n",
    "\n",
    "# if this_config['image_generation']['image_source'] == 'diffusion' and 'lmd' in this_config['image_generation']['diffusion_model_name']:\n",
    "lm_grounded_diffusion_in_context_prompt = open(this_config['image_generation']['generation']['in_context_examples_file']).read().strip()\n",
    "lm_grounded_diffusion_system_prompt = open(this_config['image_generation']['generation']['prompt_file']).read().strip()\n",
    "\n",
    "\n",
    "base_prompt = get_base_prompt(this_config['visual_program_generator']['generation']['prompt_file'],\n",
    "                              this_config['visual_program_generator']['generation']['in_context_examples_file'],\n",
    "                              this_config['visual_program_generator']['generation']['num_in_context_examples']\n",
    "                              )\n",
    "program_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    codex_model_name,  token=os.getenv('HF_ACCESS_TOKEN'), trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from vision_processes import forward\n",
    "from main_simple_lib import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sampler = TextSampler(model_name = this_config['unit_test_sampling']['model_name'],\n",
    "            sampling_strategy=this_config['unit_test_sampling']['strategy'],\n",
    "            filter_long_answers=this_config['unit_test_sampling']['filter_long_answers']\n",
    "            )\n",
    "this_config['image_generation']['return_image'] = True\n",
    "image_generator = initialize_image_generator(this_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions for Unit Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unit_tests(query,program=None, num_unit_tests=3):\n",
    "    print(\"Generating unit tests...\")\n",
    "    prompts = get_unit_test_prompt(\n",
    "                    [query], \n",
    "                    unit_test_system_prompt,\n",
    "                    unit_test_in_context_examples, \n",
    "                    llm_model_name, \n",
    "                    llm_tokenizer\n",
    "                    )\n",
    "    output = forward(model_name='llm_general', prompt=copy.deepcopy(prompts), queues=None, min_new_tokens=10, max_new_tokens=180, return_full_text=False, top_p=0.9, do_sample=True, num_return_sequences=3)\n",
    "    output = output.split('assistant')[-1]\n",
    "    unit_tests = extract_unit_tests(output)\n",
    "    if len(unit_tests) == 0:\n",
    "        output = forward(model_name='llm_general', prompt=copy.deepcopy(prompts), queues=None, min_new_tokens=10, max_new_tokens=180, return_full_text=False, top_p=0.9, do_sample=True, num_return_sequences=3)\n",
    "        output = output.split('assistant')[-1]\n",
    "        unit_tests = extract_unit_tests(output)\n",
    "    print(\"Sampling unit tests...\")\n",
    "    unit_tests = text_sampler.sample(unit_tests, num_unit_tests)\n",
    "    if this_config['image_generation']['image_source'] == 'diffusion' and 'lmd' in this_config['image_generation']['diffusion_model_name']:\n",
    "        grounded_diffusion_prompt = get_grounded_diffusion_prompt(\n",
    "                        [ut[0].replace('\"', '') for ut in unit_tests],\n",
    "                        lm_grounded_diffusion_system_prompt,\n",
    "                        lm_grounded_diffusion_in_context_prompt, \n",
    "                        llm_model_name, \n",
    "                        llm_tokenizer)\n",
    "        llm_response = []\n",
    "        print(\"Generating grounded diffusion prompts...\")\n",
    "        for p in tqdm(grounded_diffusion_prompt):\n",
    "            llm_response.append(forward(model_name='llm_general', prompt=[p], queues=None, min_new_tokens=10, max_new_tokens=320, return_full_text=False).split('assistant')[-1])\n",
    "        images = image_generator.batch_fetch_image([ut[0] for ut in unit_tests], llm_response)\n",
    "    else:\n",
    "        print(\"Generating images...\")\n",
    "        images = image_generator.batch_fetch_image([ut[0] for ut in unit_tests])\n",
    "    im_per_test = this_config['image_generation']['return_k']\n",
    "    images = [images[i*im_per_test:(i+1)*im_per_test] for i in range(len(unit_tests))]\n",
    "    \n",
    "    return unit_tests, images\n",
    "\n",
    "def get_program(query, **kwargs):\n",
    "    output = forward(model_name='codellama', prompt=[query], queues=None, **kwargs)\n",
    "    program = [extract_python_code(o) for o  in output]\n",
    "    return program\n",
    "\n",
    "\n",
    "def exec_code(image, code):\n",
    "    code = extract_python_code(code)\n",
    "    if \"def execute_command(image)\" not in code:\n",
    "        code = \"def execute_command(image):\\n\" + code\n",
    "    code = ast.unparse(ast.parse(code))\n",
    "    syntax_2 = Syntax(code, \"python\", theme=\"light\", line_numbers=True, start_line=0)\n",
    "    code = code.replace(\"def execute_command(image)\",\"def execute_command(image, my_fig, time_wait_between_lines, syntax)\") \n",
    "    print(code)\n",
    "    print('-------------------')\n",
    "    if isinstance(image, Image.Image):\n",
    "        img = image\n",
    "    elif 'http' in image:\n",
    "        img = Image.open(requests.get(image, stream=True).raw)\n",
    "    else:\n",
    "        img = Image.open(image)\n",
    "    if img.mode != 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "    try:\n",
    "        execute_code((code, syntax_2), img, show_intermediate_steps=True)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "    \n",
    "def run_fixed_code(query, image):\n",
    "    fixed_code = get_fixed_code('GQA').format(query)\n",
    "    \n",
    "    exec_code(image, fixed_code)\n",
    "\n",
    "def print_unit_tests(query,unit_tests, images):\n",
    "    print(unit_tests, images)\n",
    "    print(\"Question: \", query)\n",
    "    for i, (ut, img) in enumerate(zip(unit_tests, images)):\n",
    "        print(f'Unit Test {i+1}: {ut[0]}, {ut[1]}')\n",
    "        for im in img:\n",
    "            if isinstance(im[1], list):\n",
    "                im = im[1][0]\n",
    "            else:\n",
    "                im = im[1]\n",
    "            im.resize((256,256)).show()\n",
    "        print('-------------------')\n",
    "\n",
    "def exec_unit_tests(code, unit_tests, images):\n",
    "    for i, (ut, img) in enumerate(zip(unit_tests, images)):\n",
    "        for im in img:\n",
    "            if isinstance(im[1], list):\n",
    "                im = im[1][0]\n",
    "            else:\n",
    "                im = im[1]\n",
    "            print(f'Unit Test {i+1}: {ut[0]}, {ut[1]}')\n",
    "            exec_code(im, code)\n",
    "        print('-------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_image = \"https://cdn.mos.cms.futurecdn.net/4TDZhQ9ZDtt4GZznENbhs7-768-80.jpg.webp\"\n",
    "demo_question = \"What color are the pillows on the dark couch?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_tests, images = get_unit_tests(demo_question, num_unit_tests=2)\n",
    "print_unit_tests(demo_question, unit_tests, images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = get_program(demo_question, num_return_sequences=3, do_sample=True, top_p=0.9, max_new_tokens=320)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for code in code:\n",
    "    exec_unit_tests(code, unit_tests, images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradio Demo UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "# Gradio function\n",
    "def generate_and_display_unit_tests(query):\n",
    "    unit_tests, images = get_unit_tests(query)\n",
    "    results_text = \"\"\n",
    "    result_images = []\n",
    "    for i, (ut, img) in enumerate(zip(unit_tests, images)):\n",
    "        result_text = f\"Unit Test {i+1}: {ut[0]} - {ut[1]}\"\n",
    "        results_text += result_text + \"\\n\\n\"\n",
    "        result_images.extend([im[1] if isinstance(im[1], Image.Image) else im[1][0] for im in img])\n",
    "    \n",
    "    return results_text, result_images\n",
    "\n",
    "\n",
    "# Create the Gradio Interface\n",
    "interface = gr.Interface(\n",
    "    fn=generate_and_display_unit_tests,\n",
    "    inputs=gr.Textbox(label=\"Enter your query:\"),\n",
    "    outputs=[gr.Markdown(), gr.Gallery(label=\"Generated Images\")],\n",
    "    title=\"Unit Test and Image Generator\",\n",
    "    description=\"Enter a query to generate unit tests and corresponding images.\"\n",
    ")\n",
    "\n",
    "\n",
    "# Launch the interface\n",
    "interface.launch(share=True, inline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
